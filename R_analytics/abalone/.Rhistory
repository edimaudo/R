packages <- c('ggplot2', 'corrplot','tidyverse','NbClust','nFactors','scales')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#2. Determine number of clusters
wss <- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 2:20) wss[i] <- sum(kmeans(df,
centers=i)$withinss)
plot(1:20, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
k <- NbClust(df, distance = "euclidean",
min.nc = 2, max.nc = 10,
method = "complete", index ="all")
print(NbClust(df, min.nc=2, max.nc=15, method="kmeans"))
fviz_nbclust(df, kmeans, method = "wss")
packages <- c('ggplot2', 'corrplot','tidyverse','NbClust','nFactors','scales',
'factoextra','cluster','psy')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
kfit <- kmeans(df, k1, nstart=25, iter.max=1000,
algorithm = c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen"),
trace=FALSE)
k1 = 2
# Generate K-mean clustering
kfit <- kmeans(df, k1, nstart=25, iter.max=1000,
algorithm = c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen"),
trace=FALSE)
#Visualize clusters
fviz_cluster(kfit, data = df, ellipse.type = "convex")+theme_minimal()
fviz_cluster(kfit, data = df, geom = "point",stand = FALSE,
ellipse.type = "norm")
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','readxl')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read_excel(file.choose()) #concrete data
glimpse(df)
summary(df)
glimpse(df)
df <- read_excel(file.choose(), sheet = "Sheet1") #concrete data
glimpse(df)
df <- read_excel(file.choose(), sheet = "Sheet1") #concrete data
glimpse(df)
summary(df)
colnames(df) <- c('cement','slag','ash','water','superplast','course_agg','fine_agg','age',"comp_strength")
missing_data <- apply(df, 2, function(x) any(is.na(x))) #no missing data
print(missing_data)
#scale data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- df[,1:8]
df_cts <- as.data.frame(lapply(df_cts, normalize))
#combine data
strength <- df[,9]
df_new <- cbind(df_cts, strength)
glimpse(df_new)
#fine tune model
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
View(df_new)
strength <- as.data.frame(df[,9])
df_new <- cbind(df_cts, strength)
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
mae <- function(error)
{
mean(abs(error))
}
#models lm
fit <- lm(strength ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$Strength
MAE <- mae(error)
R2=summary(fit)$r.squared
mae <- function(error)
{
mean(abs(error))
}
mae <- function(error)
{
mean(abs(error))
}
#models lm
fit <- lm(strength ~., data=train)
summary(fit)
View(df_new)
#models lm
fit <- lm(comp_strength ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$comp_strength
MAE <- mae(error)
R2=summary(fit)$r.squared
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(train[,1:length(df_new)], train[,9], sizes=c(1:length(df_new)), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
pred1 <- predict(results, newdata = test)
error <- pred1 - test$comp_strength
MAE2 <- mae(error)
R22=summary(fit)$r.squared
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','readxl')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read.table(file.choose()) #data
glimpse(df)
colnames(df) <- c('mpg','cylinders','displacement','horsepower','weight','acceleration','model_year',
'origin','car_name')
summary(df)
df['car_name'] <- NULL
df_cat <- df[,c(2,7)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
glimpse(df_cat_new)
df_cat_new[5] <- NULL
df_cat_new[4]] <- NULL
df_cat_new[4] <- NULL
df_cat <- df[,c(2,7,8)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
glimpse(df_cat_new)
#drop columns
df_cat_new[21] <- NULL
df_cat_new[5] <- NULL
df_cat_new[4] <- NULL
#scale data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- df[,c(3,4,5,6)]
df_cts <- as.data.frame(lapply(df_cts, normalize))
df_cts <- as.data.frame(lapply(df_cts, normalize))
View(df_cts)
#scale data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- df[,c(3,4,5,6)]
df_cts <- as.data.frame(lapply(df_cts, normalize))
df_cts <- df[,c(3,4,5,6)]
df_cts <- lapply(df_cts, function(x) as.double(as.character(x)))
df_cts <- df[,c(3,4,5,6)]
df_cts <- lapply(df_cts, function(x) as.double(x))
df_cts <- as.data.frame(lapply(df_cts, normalize))
mpg <- as.data.frame(df[,c(1)])
df_new <- cbind(df_cat_new,df_cts, mpg)
#fine tune model
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
#split data
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
mae <- function(error)
{
mean(abs(error))
}
#models lm
fit <- lm(comp_strength ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$comp_strength
MAE <- mae(error)
R2=summary(fit)$r.squared
#split data
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
mae <- function(error)
{
mean(abs(error))
}
#models lm
fit <- lm(mpg ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$comp_strength
MAE <- mae(error)
R2=summary(fit)$r.squared
#models lm
fit <- lm(mpg ~., data=train)
summary(fit)
View(df_new)
mpg <- as.data.frame(df[1])
df_new <- cbind(df_cat_new,df_cts, mpg)
#fine tune model
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
View(df_new)
mae <- function(error)
{
mean(abs(error))
}
#models lm
fit <- lm(mpg ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$mpg
MAE <- mae(error)
R2=summary(fit)$r.squared
#rfe
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(train[,1:length(df_new)], train[,length(df_new)], sizes=c(1:length(df_new)), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
pred1 <- predict(results, newdata = test)
error <- pred1 - test$mpg
MAE2 <- mae(error)
R22=summary(fit)$r.squared
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','readxl')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read_excel(file.choose()) #data
glimpse(df)
summary(df)
View(df)
install.packages("devtools")
devtools::install_github("rstudio/keras")
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
install_tensorflow(gpu=TRUE)
install_tensorflow()
install_keras(tensorflow = "gpu")
library(tensorflow)
install_tensorflow(version = "1.1-gpu")
#one hot encode X6,X8
df_cat <- df[,c(6,8)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','readxl')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#one hot encode X6,X8
df_cat <- df[,c(6,8)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','readxl','dummies')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
heat <- as.data.frame(df[9])
cold <- as.data.frame(df[10])
df_cat <- df[,c(6,8)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
View(df_cat_new)
df_cat_new[10] <- NULL
df_cat_new[4] <- NULL
#scale data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- df[,c(1,2,3,4,5,7)]
df_cts <- lapply(df_cts, function(x) as.double(x))
df_cts <- as.data.frame(lapply(df_cts, normalize))
#scale data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- df[,c(1,2,3,4,5,7)]
df_cts <- lapply(df_cts, function(x) as.double(x))
df_cts <- as.data.frame(lapply(df_cts, normalize))
df_new <- cbind(df_cat_new,df_cts, heat)
#fine tune model
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
mae <- function(error)
{
mean(abs(error))
}
#models lm
fit <- lm(mpg ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$heat
MAE <- mae(error)
R2=summary(fit)$r.squared
mae <- function(error)
{
mean(abs(error))
}
#models lm
fit <- lm(heat ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$heat
MAE <- mae(error)
R2=summary(fit)$r.squared
#models lm
fit <- lm(heat ~., data=train)
summary(fit)
View(df_new)
#models lm
fit <- lm(Y1 ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$Y1
MAE <- mae(error)
R2=summary(fit)$r.squared
#rfe
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(train[,1:length(df_new)], train[,length(df_new)], sizes=c(1:length(df_new)), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
pred1 <- predict(results, newdata = test)
error <- pred1 - test$Y1
MAE2 <- mae(error)
R22=summary(pred1)$r.squared
summary(pred1)
pred1
#==================
#predict cooling
#==================
fit <- lm(Y2 ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$Y1
MAE <- mae(error)
R2=summary(fit)$r.squared
#rfe
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(train[,1:length(df_new)], train[,length(df_new)], sizes=c(1:length(df_new)), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
#get accuracy and MAE
pred1 <- predict(results, newdata = test)
error <- pred1 - test$Y2
MAE2 <- mae(error)
MAE2 <- mae(error)
pred1
#one hot encode X6,X8
df_cat <- df[,c(6,8)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
#drop columns
df_cat_new[10] <- NULL
df_cat_new[4] <- NULL
#scale data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- df[,c(1,2,3,4,5,7)]
df_cts <- lapply(df_cts, function(x) as.double(x))
df_cts <- as.data.frame(lapply(df_cts, normalize))
#combine data
df_new <- cbind(df_cat_new,df_cts, cold)
#fine tune model
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
#split data
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
fit <- lm(Y2 ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$Y1
MAE <- mae(error)
R2=summary(fit)$r.squared
fit <- lm(Y2 ~., data=train)
summary(fit)
#get accuracy and MSE
pred1 <- predict(fit, newdata = test)
error <- pred1 - test$Y2
MAE <- mae(error)
R2=summary(fit)$r.squared
#rfe
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(train[,1:length(df_new)], train[,length(df_new)], sizes=c(1:length(df_new)), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
#get accuracy and MAE
pred1 <- predict(results, newdata = test)
error <- pred1 - test$Y2
MAE2 <- mae(error)
