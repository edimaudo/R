library(NbClust)
library(ggfortify)
library(ggplot2)
#Clear old data
rm(list=ls())
#Load data using file chooser
mydata <- read.csv(file.choose(), sep = ',')
#Prepare data
mydata.orig = mydata #save orig data copy
mydata <- na.omit(mydata) # listwise deletion of missing
mydata[1] <- NULL #remove the first column
#set seed
set.seed(1)
#find the optimal # of factors
ev <- eigen(cor(mydata)) # get eigenvalues
ap <- parallel(subject=nrow(mydata),var=ncol(mydata), rep=100, cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS)
n.factors <- 11
# factor analysis
fit <- factanal(mydata,
n.factors,# number of factors to extract
scores=c("regression"),
rotation="none")
# plot factors
load <- fit$loadings[,1:n.factors]
plot(load,type="n") # set up plot
text(load,labels=names(mydata),cex=.7)
#rotate
fit <- factanal(mydata,
n.factors,# number of factors to extract
scores=c("regression"),
rotation="varimax")
# plot factors
load <- fit$loadings[,1:n.factors]
plot(load,type="n") # set up plot
text(load,labels=names(mydata),cex=.7)
#find optimal number of clusters
#using wss
fviz_nbclust(load, kmeans, method = "wss")
#using average silhouette
k.max <- 15
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 15
for(i in 2:k.max){
km.res <- kmeans(load, centers = i, nstart = 25)
ss <- silhouette(km.res$cluster, dist(load))
sil[i] <- mean(ss[, 3])
}
# Plot the  average silhouette width
plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)
# Use optimal no. of clusters in k-means #
k1=4
# Generate K-mean clustering
kfit <- kmeans(load, k1, nstart=25, iter.max=1000,
algorithm = c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen"),
trace=FALSE)
#Visualize clusters
fviz_cluster(kfit, data = load, ellipse.type = "convex")+theme_minimal()
fviz_cluster(kfit, data = load, geom = "point",stand = FALSE,
ellipse.type = "norm")
#Get cluster means and median
load_mean <- aggregate(load,by=list(kfit$cluster),FUN=mean)
load_median <- aggregate(load,by=list(kfit$cluster),FUN=median)
# append cluster assignment
loadAssigned <- data.frame(load, kfit$cluster)
# Cluster Plot against 1st 2 principal components
# vary parameters for most readable graph
library(cluster)
clusplot(load, kfit$cluster, color=TRUE, shade=TRUE,labels=2, lines=0)
#Perceptual map
pc.cr <-princomp(load, cor = TRUE) #scale data
summary(pc.cr)
biplot(pc.cr)
##Principal Component Analysis##
mydata.rev = mydata.orig #Retrieve 65 Attitudes
mydata.rev[1] <- NULL #remove the first column
#Perform PCA using prcomp() function
pca <- prcomp(mydata.rev)
#plot the variance in each component:
plot(pca, type="l")
# Manipulate data for PCA Analyis
pca.fortify <- fortify(pca)
#Set the # of clusters
k1=3
# K-Means Cluster Analysis
fit <- kmeans(mydata.rev, k1) # k1 cluster solution
# Add group column using k1=4 groups
pca4.dat <- cbind(pca.fortify, group=fit$cluster)
#Plot Kmeans Cluster:
gg2 <- ggplot(pca4.dat) +
geom_point(aes(x=PC1, y=PC2, col=factor(group), text=rownames(pca4.dat)), size=2) +
labs(title = "Visualizing K-Means Clusters Against First Two Principal Components") +
scale_color_brewer(name="", palette = "Set1")
gg2
##Get the Questions that Pertain to PCA1 and PCA2:
options(max.print=1000000) #expand the # of output to 1000000
otherPCA <- princomp(mydata.rev) #computer for PCA again
otherPCA$loadings #get info on all components including component 1 & 2
#another version of factor analysis using fa function
#library(psych)
#factoranalysis <- fa(r = cor(mydata), nfactors =
#                       n.factors, rotate = "oblimin", fm = "pa")
#plot(factoranalysis,labels=names(mydata),cex=.7, ylim=c(-.1,1))
#factor analysis
#install.packages("mclust")
#install.packages("corrplot")
#install.packages("psy")
#install.packages("nFactors")
#install.packages("GPArotation")
#install.packages("ggfortify")
#install.packages("ggplot2")
library(psy)
library(nFactors)
library(MASS)
library(psych)
library(GPArotation)
library(cluster)
library(ggplot2)
library(factoextra)
library(corrplot)
library(lattice)
library(NbClust)
library(ggfortify)
library(ggplot2)
#Clear old data
rm(list=ls())
#Load data using file chooser
mydata <- read.csv(file.choose(), sep = ',')
#Prepare data
mydata.orig = mydata #save orig data copy
mydata <- na.omit(mydata) # listwise deletion of missing
mydata[1] <- NULL #remove the first column
#set seed
set.seed(1)
#find the optimal # of factors
ev <- eigen(cor(mydata)) # get eigenvalues
ap <- parallel(subject=nrow(mydata),var=ncol(mydata), rep=100, cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS)
n.factors <- 11
# factor analysis
fit <- factanal(mydata,
n.factors,# number of factors to extract
scores=c("regression"),
rotation="none")
# plot factors
load <- fit$loadings[,1:n.factors]
plot(load,type="n") # set up plot
text(load,labels=names(mydata),cex=.7)
#rotate
fit <- factanal(mydata,
n.factors,# number of factors to extract
scores=c("regression"),
rotation="varimax")
# plot factors
load <- fit$loadings[,1:n.factors]
plot(load,type="n") # set up plot
text(load,labels=names(mydata),cex=.7)
#find optimal number of clusters
#using wss
fviz_nbclust(load, kmeans, method = "wss")
#using average silhouette
k.max <- 15
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 15
for(i in 2:k.max){
km.res <- kmeans(load, centers = i, nstart = 25)
ss <- silhouette(km.res$cluster, dist(load))
sil[i] <- mean(ss[, 3])
}
# Plot the  average silhouette width
plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)
# Use optimal no. of clusters in k-means #
k1=4
# Generate K-mean clustering
kfit <- kmeans(load, k1, nstart=25, iter.max=1000,
algorithm = c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen"),
trace=FALSE)
#Visualize clusters
fviz_cluster(kfit, data = load, ellipse.type = "convex")+theme_minimal()
fviz_cluster(kfit, data = load, geom = "point",stand = FALSE,
ellipse.type = "norm")
#Get cluster means and median
load_mean <- aggregate(load,by=list(kfit$cluster),FUN=mean)
load_median <- aggregate(load,by=list(kfit$cluster),FUN=median)
# append cluster assignment
loadAssigned <- data.frame(load, kfit$cluster)
# Cluster Plot against 1st 2 principal components
# vary parameters for most readable graph
library(cluster)
clusplot(load, kfit$cluster, color=TRUE, shade=TRUE,labels=2, lines=0)
#Perceptual map
pc.cr <-princomp(load, cor = TRUE) #scale data
summary(pc.cr)
biplot(pc.cr)
##Principal Component Analysis##
mydata.rev = mydata.orig #Retrieve 65 Attitudes
mydata.rev[1] <- NULL #remove the first column
#Perform PCA using prcomp() function
pca <- prcomp(mydata.rev)
#plot the variance in each component:
plot(pca, type="l")
# Manipulate data for PCA Analyis
pca.fortify <- fortify(pca)
#Set the # of clusters
k1=3
# K-Means Cluster Analysis
fit <- kmeans(mydata.rev, k1) # k1 cluster solution
# Add group column using k1=4 groups
pca4.dat <- cbind(pca.fortify, group=fit$cluster)
#Plot Kmeans Cluster:
gg2 <- ggplot(pca4.dat) +
geom_point(aes(x=PC1, y=PC2, col=factor(group), text=rownames(pca4.dat)), size=2) +
labs(title = "Visualizing K-Means Clusters Against First Two Principal Components") +
scale_color_brewer(name="", palette = "Set1")
gg2
##Get the Questions that Pertain to PCA1 and PCA2:
options(max.print=1000000) #expand the # of output to 1000000
otherPCA <- princomp(mydata.rev) #computer for PCA again
otherPCA$loadings #get info on all components including component 1 & 2
#another version of factor analysis using fa function
#library(psych)
#factoranalysis <- fa(r = cor(mydata), nfactors =
#                       n.factors, rotate = "oblimin", fm = "pa")
#plot(factoranalysis,labels=names(mydata),cex=.7, ylim=c(-.1,1))
#factor analysis
#install.packages("mclust")
#install.packages("corrplot")
#install.packages("psy")
#install.packages("nFactors")
#install.packages("GPArotation")
#install.packages("ggfortify")
#install.packages("ggplot2")
library(psy)
library(nFactors)
library(MASS)
library(psych)
library(GPArotation)
library(cluster)
library(ggplot2)
library(factoextra)
library(corrplot)
library(lattice)
library(NbClust)
library(ggfortify)
library(ggplot2)
#Clear old data
rm(list=ls())
#Load data using file chooser
mydata <- read.csv(file.choose(), sep = ',')
#Prepare data
mydata.orig = mydata #save orig data copy
mydata <- na.omit(mydata) # listwise deletion of missing
mydata[1] <- NULL #remove the first column
#set seed
set.seed(1)
#find the optimal # of factors
ev <- eigen(cor(mydata)) # get eigenvalues
ap <- parallel(subject=nrow(mydata),var=ncol(mydata), rep=100, cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS)
n.factors <- 11
# factor analysis
fit <- factanal(mydata,
n.factors,# number of factors to extract
scores=c("regression"),
rotation="none")
# plot factors
load <- fit$loadings[,1:n.factors]
plot(load,type="n") # set up plot
text(load,labels=names(mydata),cex=.7)
#rotate
fit <- factanal(mydata,
n.factors,# number of factors to extract
scores=c("regression"),
rotation="varimax")
# plot factors
load <- fit$loadings[,1:n.factors]
plot(load,type="n") # set up plot
text(load,labels=names(mydata),cex=.7)
#find optimal number of clusters
#using wss
fviz_nbclust(load, kmeans, method = "wss")
#using average silhouette
k.max <- 15
sil <- rep(0, k.max)
# Compute the average silhouette width for
# k = 2 to k = 15
for(i in 2:k.max){
km.res <- kmeans(load, centers = i, nstart = 25)
ss <- silhouette(km.res$cluster, dist(load))
sil[i] <- mean(ss[, 3])
}
# Plot the  average silhouette width
plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Number of clusters k")
abline(v = which.max(sil), lty = 2)
# Use optimal no. of clusters in k-means #
k1=4
# Generate K-mean clustering
kfit <- kmeans(load, k1, nstart=25, iter.max=1000,
algorithm = c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen"),
trace=FALSE)
#Visualize clusters
fviz_cluster(kfit, data = load, ellipse.type = "convex")+theme_minimal()
fviz_cluster(kfit, data = load, geom = "point",stand = FALSE,
ellipse.type = "norm")
#Get cluster means and median
load_mean <- aggregate(load,by=list(kfit$cluster),FUN=mean)
load_median <- aggregate(load,by=list(kfit$cluster),FUN=median)
# append cluster assignment
loadAssigned <- data.frame(load, kfit$cluster)
# Cluster Plot against 1st 2 principal components
# vary parameters for most readable graph
library(cluster)
clusplot(load, kfit$cluster, color=TRUE, shade=TRUE,labels=2, lines=0)
#Perceptual map
pc.cr <-princomp(load, cor = TRUE) #scale data
summary(pc.cr)
biplot(pc.cr)
##Principal Component Analysis##
mydata.rev = mydata.orig #Retrieve 65 Attitudes
mydata.rev[1] <- NULL #remove the first column
#Perform PCA using prcomp() function
pca <- prcomp(mydata.rev)
#plot the variance in each component:
plot(pca, type="l")
# Manipulate data for PCA Analyis
pca.fortify <- fortify(pca)
#Set the # of clusters
k1=3
# K-Means Cluster Analysis
fit <- kmeans(mydata.rev, k1) # k1 cluster solution
# Add group column using k1=4 groups
pca4.dat <- cbind(pca.fortify, group=fit$cluster)
#Plot Kmeans Cluster:
gg2 <- ggplot(pca4.dat) +
geom_point(aes(x=PC1, y=PC2, col=factor(group), text=rownames(pca4.dat)), size=2) +
labs(title = "Visualizing K-Means Clusters Against First Two Principal Components") +
scale_color_brewer(name="", palette = "Set1")
gg2
##Get the Questions that Pertain to PCA1 and PCA2:
options(max.print=1000000) #expand the # of output to 1000000
otherPCA <- princomp(mydata.rev) #computer for PCA again
otherPCA$loadings #get info on all components including component 1 & 2
#another version of factor analysis using fa function
#library(psych)
#factoranalysis <- fa(r = cor(mydata), nfactors =
#                       n.factors, rotate = "oblimin", fm = "pa")
#plot(factoranalysis,labels=names(mydata),cex=.7, ylim=c(-.1,1))
##Principal Component Analysis##
mydata.rev = mydata.orig #Retrieve 65 Attitudes
mydata.rev[1] <- NULL #remove the first column
#Perform PCA using prcomp() function
pca <- prcomp(mydata.rev)
#plot the variance in each component:
plot(pca, type="l")
summary(pca)
plot(pc, type='l')
scree.plot(pc,type="line",main ="Scree plot")
plot(pca, type='l')
scree.plot(pca,type="line",main ="Scree plot")
plot(pca, type='l')
scree.plot(pca,type="line",main ="Scree plot")
comp <- data.frame(pca$x[,1:5]) #<-- very important 1:5
# Plot
plot(comp, pch=16, col=rgb(0,0,0,0.5))
pc = princomp(mydata.rev, scores=TRUE, cor = TRUE)
summary(pc)
loadings(pc)
#plot of eigenvalues
plot(pc)
plot(pc, type='l')
scree.plot(pc,type="line",main ="Scree plot")
#biplot
biplot(pc)
#component scores
pc$scores[1:2000,]
pca4.dat
install.packages(mlogit)
install.packages("mlogit")
#Predicting house prices
#load libraries
library(MASS)
library(Metrics)
library(corrplot)
library(randomForest)
library(lars)
library(ggplot2)
library(xgboost)
library(Matrix)
library(methods)
install.packages("xgboost")
install.packages("randomForest")
#Predicting house prices
#load libraries
library(MASS)
library(Metrics)
library(corrplot)
library(randomForest)
library(lars)
library(ggplot2)
library(xgboost)
library(Matrix)
library(methods)
#load data
Training <- read.csv("train.csv")
Test <- read.csv("test.csv")
#set up work process
#Data cleaning, Descriptive Analysis, Model Selection, Final Prediction
#data cleaning
Num_NA<-sapply(Training,function(y)length(which(is.na(y)==T)))
NA_Count<- data.frame(Item=colnames(Training),Count=Num_NA)
#remove columns that have high number of missing values
Training<- Training[,-c(7,73,74,75)]
#ransferred dummny variables into numeric form
# Numeric Variables
Num<-sapply(Training,is.numeric)
Num<-Training[,Num]
for(i in 1:77){
if(is.factor(Training[,i])){
Training[,i]<-as.integer(Training[,i])
}
}
# Test
Training$Street[1:50]
Training[is.na(Training)]<-0
Num[is.na(Num)]<-0
#Descriptive analysis
setwd("~/Documents/Coding/R/R_analytics/R_housing_prediction")
#Predicting house prices
#load libraries
library(MASS)
library(Metrics)
library(corrplot)
library(randomForest)
library(lars)
library(ggplot2)
library(xgboost)
library(Matrix)
library(methods)
#load data
Training <- read.csv("train.csv")
Test <- read.csv("test.csv")
#set up work process
#Data cleaning, Descriptive Analysis, Model Selection, Final Prediction
#data cleaning
Num_NA<-sapply(Training,function(y)length(which(is.na(y)==T)))
NA_Count<- data.frame(Item=colnames(Training),Count=Num_NA)
#remove columns that have high number of missing values
Training<- Training[,-c(7,73,74,75)]
#ransferred dummny variables into numeric form
# Numeric Variables
Num<-sapply(Training,is.numeric)
Num<-Training[,Num]
for(i in 1:77){
if(is.factor(Training[,i])){
Training[,i]<-as.integer(Training[,i])
}
}
# Test
Training$Street[1:50]
Training[is.na(Training)]<-0
Num[is.na(Num)]<-0
#Descriptive analysis
#correlation of numerical values
correlations<- cor(Num[,-1],use="everything")
corrplot(correlations, method="circle", type="lower",  sig.level = 0.01, insig = "blank")
pairs(~SalePrice+OverallQual+TotalBsmtSF+GarageCars+GarageArea,data=Training,
main="Scatterplot Matrix")
p<- ggplot(Training,aes(x= YearBuilt,y=SalePrice))+geom_point()+geom_smooth()
p
reg1<- lm(SalePrice~., data = Training_Inner)
summary(reg1)
#split the training dataset in the ration 6:4
Training_Inner<- Training[1:floor(length(Training[,1])*0.6),]
Test_Inner<- Training[(length(Training_Inner[,1])+1):1460,]
#linear regression
reg1<- lm(SalePrice~., data = Training_Inner)
summary(reg1)
reg1_Modified_2<-lm(formula = SalePrice ~ MSSubClass + LotArea +
Condition2 + OverallQual + OverallCond +
YearBuilt  + RoofMatl +  ExterQual +
BsmtQual + BsmtCond + BsmtFinSF1 + BsmtFinSF2 +
BsmtUnfSF + X1stFlrSF + X2ndFlrSF + BedroomAbvGr + KitchenAbvGr +
KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + FireplaceQu +
GarageYrBlt + GarageCars +  SaleCondition,
data = Training_Inner)
summary(reg1_Modified_2)
Prediction_1<- predict(reg1_Modified_2, newdata= Test_Inner)
rmse(log(Test_Inner$SalePrice),log(Prediction_1))
rmse(log(Test_Inner$SalePrice),log(Prediction_1))
rmse(log(Test_Inner$SalePrice),log(Prediction_1))
rmse <- function(error)
{
sqrt(mean(error^2))
}
rmse(log(Test_Inner$SalePrice),log(Prediction_1))
load.libraries <- c('data.table', 'testthat', 'gridExtra', 'corrplot', 'GGally', 'ggplot2', 'e1071', 'dplyr')
install.lib <- load.libraries[!load.libraries %in% installed.packages()]
for(libs in install.lib) install.packages(libs, dependences = TRUE)
sapply(load.libraries, require, character = TRUE)
cat_var <- names(train)[which(sapply(train, is.character))]
cat_car <- c(cat_var, 'BedroomAbvGr', 'HalfBath', ' KitchenAbvGr','BsmtFullBath', 'BsmtHalfBath', 'MSSubClass')
numeric_var <- names(train)[which(sapply(train, is.numeric))]
