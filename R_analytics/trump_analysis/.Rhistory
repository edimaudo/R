df$PRICE_PLAN_CODE <- NULL
df$rateplan_name <- NULL
df$activation_district <- NULL
df$activation_region  <- NULL
df$last_transaction_district  <- NULL
df$last_transaction_region <- NULL
glimpse(df)
df_cat <- df[,c(1)]
#cts data
df_cts <- df[,-c(df_cat)]
df_cat <- df[,c(1)]
#cts data
df_cts <- df[,-(df_cat)]
df_cts <- df[,-df_cat]
val <- c(1)
df_cts <- df[,-val]
df_user <- read.csv(file.choose(), header = TRUE)
df_trips <- read.csv(file.choose(), header = TRUE)
df_transactions <- read.csv(file.choose(), header = TRUE)
glimpse(df_user)
#package
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','dummies')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
glimpse(df_user)
glimpse(df_trips)
glimpse(df_transactions)
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','dummies')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read.csv(file.choose(), sep = ";")
glimpse(df)
summary(df)
#recode y
df[,21] <- ifelse(df[,21] == "no", "0", "1")
df[,21] <- as.factor(df[,21])
y = df[,21]
#recode categorical
df_cat <- df[,c(2,3,4,5,6,7,8,9,10,15)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
#normalize cts variables
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- df[,c(1,11,12,13,14,16,17,18,19,20)]
df_cts <- as.data.frame(lapply(df_cts, normalize))
#combine data
#combine data
df_new <- cbind(df_cat_new,df_cts, y)
#fine tune model
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
#split data
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
df.backup <- df
control <- trainControl(method="repeatedcv", number=10, repeats=3)
#modelnames <- paste(names(getModelInfo()), collapse=',  ') #get model names
#adaboost
#fit.adaboost <- train(y~., data=train, method="adaboost", trControl=control)
#adabag
#fit.adabag <- train(y~., data=train, method="adaBag", trControl=control)
#random forest
#fit.rf <- train(y~., data=train, method="rf", trControl=control)
#boosting algorithm - Stochastic Gradient Boosting (Generalized Boosted Modeling)
fit.gbm <- train(y~., data=train, method="gbm", trControl=control)
#svm
#fit.svm <- train(y~., data=train, method="svmRadial", trControl=control)
#gbm h20
fit.gbmh2o <- train(y~., data=train, method="gbm_h2o", trControl=control)
#------------------
#compare models
#------------------
results <- resamples(list(gradboost = fit.gbm, gbmh20 = fit.gbmh2o))
summary(results)
# boxplot comparison
bwplot(results)
# Dot-plot comparison
dotplot(results)
results <- resamples(list(gradboost = fit.gbm))
test_scores <- predict(fit.gbm, test)
confusionMatrix(test_scores, test$Target)
confusionMatrix(test_scores, test$y)
View(df)
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','readxl')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read.csv(file.choose(), sep = ";")
View(df)
df <- read.tsv(file.choose(), sep = ";")
df <- read_tsv(file.choose(), sep = ";")
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','readxl','readr')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read_tsv(file.choose(), sep = ";")
df <- read_tsv(file.choose())
View(df)
df <- read.delim(file.choose(), sep="\t",stringsAsFactors = FALSE ,header=FALSE)
View(df)
head(df)
colnames(df) <- c("DateOccurred","DateReported",'Location','ShortDescription',"Duration",
"LongDescription")
df$DateOccurred <- as.Date(df$DateOccurred,format = "%Y%m%d")
#rfm model
#clear old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','readxl',
'lubridate','')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#load excel file
df <- as.data.frame(read_excel(file.choose()))
glimpse(df)
#rename columns
colnames(df) <- c("TRANDATE","SALESTXN","CUSTNO")
#do rfm model
rfm_data <- df %>%
select(CUSTNO,TRANDATE,SALESTXN) %>%
drop_na()
#create real rfm data
rfm_temp <- rfm_data %>%
group_by(CUSTNO) %>%
summarise(recency=(as.numeric(as.Date(today())-max(TRANDATE))),
frequency=n(), monetary= sum(SALESTXN))
View(rfm_temp)
#convert date
rfm_data <- rfm_data %>%
mutate(TRANDATE = as.Date(TRANDATE,"%Y-%m-%d"))
#create real rfm data
rfm_temp <- rfm_data %>%
group_by(CUSTNO) %>%
summarise(recency=(as.numeric(as.Date(today())-max(TRANDATE))),
frequency=n(), monetary= sum(SALESTXN))
#rfm rank scoring
# sort data set for Recency with Recency (ascending) - Frequency (descending) - Monetary (descending)
rfm_temp <- rfm_temp %>%
arrange(recency, desc(frequency), desc(monetary))
rfm_temp <- rfm_temp %>%
mutate(recency = 0-recency)
rfm_temp$rankR <- cut(rfm_temp$recency,5,labels=F)
#change recency back to normal
rfm_temp <- rfm_temp %>%
mutate(recency = 0-recency)
# sort data set for Frequency with Recency (descending) - Frequency (ascending) - Monetary (descending)
rfm_temp <- rfm_temp %>%
arrange(desc(recency), frequency, desc(monetary))
rfm_temp$rankF <- cut(rfm_temp$frequency,5,labels=F)
# sort data set for Monetary with Recency (descending) - Frequency (descending) - Monetary (ascending)
rfm_temp <- rfm_temp %>%
arrange(desc(recency), desc(frequency), monetary)
rfm_temp$rankM <- cut(rfm_temp$monetary,5,labels=F)
#combine rfm into single score
rfm_temp$Score <- with(rfm_temp, paste0(rankR, rankF, rankM))
rfm_temp <- rfm_temp %>%
mutate(Score = as.integer(Score))
#get output sorted by score
rfm_temp <- rfm_temp %>%
arrange(desc(Score))
View(rfm_temp)
require(lpSolve)
install.packages("lpSolve")
#LP MODEL
#8A + 7B
#3A + 8B >= 6
#6A + 4B >= 4.5
#4A + 6B >= 5
library(lpSolve)
#decision variables
C <- c(8,7)
#constraint matrix
A <- matrix(c(3,8,6,4,4,6),nrow = 3,byrow = TRUE)
B <- c(6,4,5,5)
constraint_direction <- c(">=",">=",">=",">=")
# Find the optimal solution
optimum <-  lp(direction="min",
objective.in = C,
const.mat = A,
const.dir = constranint_direction,
const.rhs = B,
all.int = T)
# Print status: 0 = success, 2 = no feasible solution
print(optimum$status)
constraint_direction <- c(">=",">=",">=",">=")
# Find the optimal solution
optimum <-  lp(direction="min",
objective.in = C,
const.mat = A,
const.dir = constraint_direction ,
const.rhs = B,
all.int = T)
# Print status: 0 = success, 2 = no feasible solution
print(optimum$status)
#decision variables
C <- c(8,7)
#constraint matrix
A <- matrix(c(3,6,4,8,4,6),nrow = 3,byrow = TRUE)
B <- c(6,4.5,5)
constraint_direction <- c(">=",">=",">=",">=")
# Find the optimal solution
optimum <-  lp(direction="min",
objective.in = C,
const.mat = A,
const.dir = constraint_direction ,
const.rhs = B,
all.int = T)
# Print status: 0 = success, 2 = no feasible solution
print(optimum$status)
#constraint matrix
A <- matrix(c(3,6,4,8,4,6),nrow = 2,byrow = TRUE)
B <- c(6,4.5,5)
constraint_direction <- c(">=",">=",">=",">=")
# Find the optimal solution
optimum <-  lp(direction="min",
objective.in = C,
const.mat = A,
const.dir = constraint_direction ,
const.rhs = B,
all.int = T)
# Print status: 0 = success, 2 = no feasible solution
print(optimum$status)
#constraint matrix
A <- matrix(c(3,6,4,8,4,6),nrow = 3,byrow = TRUE)
B <- c(6,4.5,5)
constraint_direction <- c(">=",">=",">=","<=")
# Find the optimal solution
optimum <-  lp(direction="min",
objective.in = C,
const.mat = A,
const.dir = constraint_direction ,
const.rhs = B,
all.int = T)
# Print status: 0 = success, 2 = no feasible solution
print(optimum$status)
#decision variables
C <- c(8,7)
#constraint matrix
A <- matrix(c(3,6,4,8,4,6),nrow = 3,byrow = TRUE)
B <- c(6,4.5,5)
constraint_direction <- c(">=",">=",">=")
# Find the optimal solution
optimum <-  lp(direction="min",
objective.in = C,
const.mat = A,
const.dir = constraint_direction ,
const.rhs = B,
all.int = T)
# Print status: 0 = success, 2 = no feasible solution
print(optimum$status)
# Check the value of objective function at optimal point
print(paste("Total cost: ", optimum$objval, sep=""))
library(readr)
df <- read_fwf("UKM00003023-data.txt", fwf_widths(c(1,1,6,7,1,5,1,5,1,5,6,6,6)))
names(df) <- c("LVLTYP1","LVLTYP2","ETIME","PRESS","PFLAG","GPH","ZFLAG","TEMP","TFLAG","RH","DPDP","WDIR","WSDP")
library(readr)
df <- read_fwf(file.choose(), fwf_widths(c(1,1,6,7,1,5,1,5,1,5,6,6,6)))
names(df) <- c("LVLTYP1","LVLTYP2","ETIME","PRESS","PFLAG","GPH","ZFLAG","TEMP","TFLAG","RH","DPDP","WDIR","WSDP")
View(df)
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','dummies','readxl')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read_excel(file.choose()) #credit data
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x))) #no missing data
print(missing_data)
x <- df[1,]
x2 <- as.data.frame(x)
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','dummies','readxl')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
df <- read_excel(file.choose()) #credit data
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x))) #no missing data
print(missing_data)
x <- df[1,]
x2 <- as.data.frame(x)
x2 <- as.data.frame(x)
#delete first row
df <- df[-c(1),]
#delete first column
df[1] <- NULL
colnamesinfo <- as.data.frame(colnames(x2[,3:24]))
colnames(colnamesinfo) <- c("columns")
View(df)
View(df)
library(xgboost)
Target <- as.factor(df$Y)
df_cat <- df[,c(2,3,4,6,7,8,9,10,11)]
#convert to factor
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
df_cts <- df[,c(5,12:23)]
#convert chr to float
df_cts <- lapply(df_cts, function(x) as.double(as.character(x)))
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- as.data.frame(lapply(df_cts, normalize))
df_new <- cbind(df_cat_new, df_cts, Target)
#remove redundant columns
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
#split data
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
#create traning and test data
set.seed(100)  # For reproducibility
# Create index for testing and training data
inTrain <- createDataPartition(y = df_new$Target, p = 0.8, list = FALSE)
# subset power_plant data to training
training <- df_new[inTrain,]
# subset the rest to test
testing <- df_new[-inTrain,]
#convert to dmatrixes
X_train = xgb.DMatrix(as.matrix(training %>% select(-Target)))
y_train = training$Target
X_test = xgb.DMatrix(as.matrix(testing %>% select(-Target)))
y_test = testing$Target
#remove old data
rm(list=ls())
#packages
packages <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust",
"cluster", "igraph", "fpc")
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
getwd()
cname <- file.path("~", "Users/edima/Documents/Coding/R/R_analytics/trump_analysis/⁩", "txts")
docs <- VCorpus(DirSource(cname))
summary(docs)
cname <- file.path("~", "/Users/edima/Documents/Coding/R/R_analytics/trump_analysis/⁩", "txts")
docs <- VCorpus(DirSource(cname))
summary(docs)
cname <- file.path("~", "/Users/edima/Documents/Coding/R/R_analytics/trump_analysis/", "texts")
cname
cname <- file.path("~", "Users/edima/Documents/Coding/R/R_analytics/trump_analysis", "texts")
cname
cname
docs <- VCorpus(DirSource(cname))
summary(docs)
cname <- file.path("~", "Users/edima/Documents/Coding/R/R_analytics", "trump_analysis")
#cname
#install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
cname <- file.path("~", "Users/edima/Documents/Coding/R/R_analytics", "trump_analysis")
cname
docs <- VCorpus(DirSource(cname))
summary(docs)
#install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
cname <- file.path("~", "Users/edima/Documents/Coding/R/R_analytics", "trump_analysis/")
cname
docs <- VCorpus(DirSource(cname))
summary(docs)
setwd("~/Documents/Coding/R/R_analytics/trump_analysis")
docs <- VCorpus(DirSource(getwd()))
summary(docs)
inspect(docs[1])
docs <- tm_map(docs,removePunctuation)
for (j in seq(docs)) {
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
DocsCopy <- docs
# remove stopwords("english")
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))
for (j in seq(docs))
{
docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
docs_stc <- tm_map(docs_st, stemCompletion, dictionary = DocsCopy, lazy=TRUE)
docs_stc <- tm_map(docs_stc, PlainTextDocument)
writeLines(as.character(docs_stc[1]))
docs_st <- tm_map(docs, stemDocument)
docs_st <- tm_map(docs_st, PlainTextDocument)
writeLines(as.character(docs_st[1])) # Check to see if it worked.
docs_stc <- tm_map(docs_st, stemCompletion, dictionary = DocsCopy, lazy=TRUE)
docs_stc <- tm_map(docs_stc, PlainTextDocument)
writeLines(as.character(docs_stc[1]))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
dtm
tdm <- TermDocumentMatrix(docs)
tdm
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
m <- as.matrix(dtm)
dim(m)
#  Start by removing sparse terms:
dtms <- removeSparseTerms(dtm, 0.2) # This makes a matrix that is 20% empty space, maximum.
dtms
freq <- colSums(as.matrix(dtm))
head(table(freq), 20)
tail(table(freq), 20)
freq <- colSums(as.matrix(dtms))
freq
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
library(ggplot2)
p <- ggplot(subset(wf, freq>50), aes(x = reorder(word, -freq), y = freq)) +
geom_bar(stat = "identity") +
theme(axis.text.x=element_text(angle=45, hjust=1))
p
findAssocs(dtm, c("country" , "american"), corlimit=0.85)
set.seed(142)
wordcloud(names(freq), freq, min.freq=25)
library(RColorBrewer)
set.seed(142)
wordcloud(names(freq), freq, min.freq=25)
dtmss <- removeSparseTerms(dtm, 0.15) # This makes a matrix that is only 15% empty space, maximum.
dtmss
library(cluster)
d <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d, method="complete")   # for a different look try substituting: method="ward.D"
fit
library(fpc)
d <- dist(t(dtmss), method="euclidian")
kfit <- kmeans(d, 2)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
