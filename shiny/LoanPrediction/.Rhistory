df_train_cts <- df_train[,c(6,7,8,9,10)]
df_train_new <- cbind(df_train_cat_new, df_train_cts,df_train$y)
Loan_status <- as.factor(df_train$Loan_Status)
df_train_new <- cbind(df_train_cat_new, df_train_cts,Loan_Status)
df_train_new <- cbind(df_train_cat_new, df_train_cts,Loan_status)
df_test_cat <- df_test[,c(1,2,3,4,5,11)]
df_test_cat_new <- dummy.data.frame(as.data.frame(df_test_cat), sep = "_")
df_test_cts <- df_test[,c(6,7,8,9,10)]
df_test_new <- cbind(df_test_cat_new, df_test_cts)
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','dummies','readxl',
'cluster','factoextra','psy','lattice','nFactors','scales','NbClust','keras')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#load data
df <- read_csv(file.choose())
glimpse(df)
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x))) #no missing data
print(missing_data)
df.backup <- df
#scale data
df[,c(1,2,3,4,5,6,7,8)] <- scale(df[,c(1,2,3,4,5,6,7,8)])
#set y as factor
df$y <- as.factor(df$y)
#split in train and test data
set.seed(123)
sample <- sample.split(df,SplitRatio = 0.70)
train <- subset(df,sample ==TRUE)
test <- subset(df, sample==FALSE)
X_train <- train[,c(1,2,3,4,5,6,7,8)]
y_train <- train$y
X_test <- test[,c(1,2,3,4,5,6,7,8)]
y_test <- test$y
install_tensorflow()
install_keras()
install_tensorflow()
install_keras()
tensorflow::install_tensorflow()
devtools::install_github("rstudio/keras")
devtools::install_github("rstudio/tensorflow")
#build model
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'sigmoid')
history <- model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
model %>% fit(
X_train, y_train,
epochs = 100,
batch_size = 5,
validation_split = 0.3
)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'sigmoid')
devtools::install_github("rstudio/keras")
library(keras)
install_keras()
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','dummies','readxl',
'cluster','factoextra','psy','lattice','nFactors','scales','NbClust','keras')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#build model
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'sigmoid')
devtools::install_github("rstudio/keras")
devtools::install_github("rstudio/tensorflow")
#build model
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'sigmoid')
#build model
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'sigmoid')
#remove old data
rm(list=ls())
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','dummies','readxl',
'cluster','factoextra','psy','lattice','nFactors','scales','NbClust','keras')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#load data
df <- read_csv(file.choose())
df.backup <- df
#scale data
df[,c(1,2,3,4,5,6,7,8)] <- scale(df[,c(1,2,3,4,5,6,7,8)])
#set y as factor
df$y <- as.factor(df$y)
#split in train and test data
set.seed(123)
sample <- sample.split(df,SplitRatio = 0.70)
train <- subset(df,sample ==TRUE)
test <- subset(df, sample==FALSE)
X_train <- train[,c(1,2,3,4,5,6,7,8)]
y_train <- train$y
X_test <- test[,c(1,2,3,4,5,6,7,8)]
y_test <- test$y
#build model
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'sigmoid')
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/tensorflow", force=TRUE)
devtools::install_github("rstudio/keras", force = TRUE)
shiny::runApp('Documents/Coding/R/shiny/LoanPrediction')
getwd()
setwd("~/Documents/Coding/R/shiny/LoanPrediction")
runApp()
setwd("~/Documents/Coding/R/shiny/LoanPrediction")
runApp()
runApp()
runApp()
#rent prediction
#remove old data
rm(list=ls())
#packages
packages <- c('ggplot2', 'corrplot','tidyverse','caret','mlbench','mice', 'caTools','dummies')
#load packages
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#load data
df <- read_csv(file.choose())
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x))) #no missing data
print(missing_data)
df <- na.omit(df)
summary(df)
df[1] <- NULL
df[2] <- NULL
glimpse(df)
c(2014, format(Sys.Date(), "%Y"))
c(2019, format(Sys.Date(), "%Y"))
(format(Sys.Date(), "%Y"))
as.integer(format(Sys.Date(), "%Y"))
df$PlaceAge <- as.integer(format(Sys.Date(), "%Y")) - df$`Year Built`
df$`Year Built` <- NULL
glimpse(df)
df <- read_csv(file.choose())
df.backup <- df
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#remove missing data
df <- na.omit(df)
unique(df$PostalCode)
#drop columns
df$`Street 1` <- NULL
df$City <- NULL
#data transformation
df$PlaceAge <- as.integer(format(Sys.Date(), "%Y")) - df$`Year Built`
#drop year build
df$`Year Built` <- NULL
glimpse(df)
View(df)
format(Sys.Date(), "%M%D%Y")
format(Sys.Date(), "M%D%Y")
format(Sys.Date(), "%D%Y")
format(Sys.Date(), "%D%YYYY")
format(Sys.Date(), "D%Y")
format(Sys.Date(), "%D/%Y")
format(Sys.Date(), "%D%Y")
format(Sys.Date(), "%D%")
format(Sys.Date(), "%D")
format(Sys.Date(), "%D") - as.Date('8/01/2017')
as.Date(format(Sys.Date(), "%D")) - as.Date('8/01/2017')
df$LeaseEndDate <- as.Date(df$LeaseEndDate)
library(lubridate)
df$LeaseEndDate <- mdy(df$LeaseEndDate)
as.Date(format(Sys.Date(), "%D")) - df$LeaseEndDate
Sys.Date() - df$LeaseEndDate
df$LeaseDiffDays <- Sys.Date() - df$LeaseEndDate
df$LeaseEndDate <- NULL
glimpse(df)
LeaseDiffDays <- as.integer(df$LeaseDiffDays)
glimpse(df)
LeaseDiffDays <- as.numeric(df$LeaseDiffDays)
glimpse(df)
df<- df.backup
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#remove missing data
df <- na.omit(df)
summary(df)
#drop columns
df$`Street 1` <- NULL
df$City <- NULL
#data transformation
df$PlaceAge <- as.integer('format(Sys.Date(), "%Y")') - df$`Year Built`
#drop year build
df$`Year Built` <- NULL
#update lease date
df$LeaseEndDate <- mdy(df$LeaseEndDate)
df$LeaseDiffDays <- as.as.numeric((Sys.Date() - df$LeaseEndDate))
df$LeaseDiffDays <- as.numeric((Sys.Date() - df$LeaseEndDate))
View(df)
df<- df.backup
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#remove missing data
df <- na.omit(df)
summary(df)
#drop columns
df$`Street 1` <- NULL
df$City <- NULL
#data transformation
df$PlaceAge <- as.integer('format(Sys.Date(), "%Y")') - df$`Year Built`
df
df <- read_csv(file.choose())
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#remove missing data
df <- na.omit(df)
summary(df)
#drop columns
df$`Street 1` <- NULL
df$City <- NULL
df$PlaceAge <- as.integer('format(Sys.Date(), "%Y")') - df$`Year Built`
glimpse(df)
#load data
df <- read_csv(file.choose())
df.backup <- df
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#remove missing data
df <- na.omit(df)
summary(df)
#drop columns
df$`Street 1` <- NULL
df$City <- NULL
#data transformation
df$PlaceAge <-as.integer(format(Sys.Date(), "%Y")) - df$`Year Built`
#drop year build
df$`Year Built` <- NULL
#update lease date
df$LeaseEndDate <- mdy(df$LeaseEndDate)
df$LeaseDiffDays <- as.numeric((Sys.Date() - df$LeaseEndDate))
#drop lease Date
df$LeaseEndDate <- NULL
glimpse(df)
df$PostalCode <- as.factor(df$PostalCode)
df$Bathrooms <- as.factor(df$Bathrooms)
df$Bedrooms <- as.factor(df$Bedrooms)
df_cat <- df[,c(1,2,3)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
df_cts <- df[,c(4,6,7)]
#convert chr to float
df_cts <- lapply(df_cts, function(x) as.double(as.character(x)))
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- as.data.frame(lapply(df_cts, normalize))
Target <- df$RecurringCharges
df_new <- cbind(df_cat_new, df_cts, Target)
library(randomForest)
#remove redundant columns
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
reg1<- lm(Target~., data = train)
summary(reg1)
rmse <- function(error)
{
sqrt(mean(error^2))
}
#check performance of model
Prediction_1<- predict(reg1, newdata= test)
rmse((test$Target),(Prediction_1))
Prediction_1<- predict(reg1, newdata= test)
View(df_new)
df$PostalCode <- NULL
glimpse(df)
df_cts <- df[,c(4,5,6)]
#convert chr to float
df_cts <- lapply(df_cts, function(x) as.double(as.character(x)))
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- as.data.frame(lapply(df_cts, normalize))
Target <- df$RecurringCharges
df_new <- cbind(df_cat_new, df_cts, Target)
#remove redundant columns
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
View(df_new)
df <- read_csv(file.choose())
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#remove missing data
df <- na.omit(df)
#drop columns
df$PostalCode <- NULL
df$`Street 1` <- NULL
df$City <- NULL
#data transformation
df$PlaceAge <-as.integer(format(Sys.Date(), "%Y")) - df$`Year Built`
#drop year build
df$`Year Built` <- NULL
#update lease date
df$LeaseEndDate <- mdy(df$LeaseEndDate)
df$LeaseDiffDays <- as.numeric((Sys.Date() - df$LeaseEndDate))
#drop lease Date
df$LeaseEndDate <- NULL
#convert catgeorical
df_cat <- df[,c(1,2)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
glimpse(df)
df_cts <- df[,c(4,5,6)]
#convert chr to float
df_cts <- lapply(df_cts, function(x) as.double(as.character(x)))
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- as.data.frame(lapply(df_cts, normalize))
Target <- df$RecurringCharges
df_new <- cbind(df_cat_new, df_cts, Target)
#remove redundant columns
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
#split data
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
reg1<- lm(Target~., data = train)
summary(reg1)
View(df_new)
rmse <- function(error)
{
sqrt(mean(error^2))
}
#check performance of model
Prediction_1<- predict(reg1, newdata= test)
rmse((test$Target),(Prediction_1))
Prediction_1<- predict(reg1, newdata= test)
rmse(test$Target,Prediction_1)
rmse(test$Target,Prediction_1)
df_new <- cbind(df_cat_new, df_cts, Target)
View(df_new)
#drop 1 and 3
df_new[1] <- NULL
df-new[3] <- NULL
#remove redundant columns
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
df_new <- df_new[,-c(highlyCorrelated)]
#split data
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
reg1<- lm(Target~., data = train)
summary(reg1)
reg2<- randomForest(Target~.,data= train)
Prediction_2 <- predict(reg2, newdata= test)
rmse(test$Target,Prediction_2)
rmse(Prediction_2,test$Target)
rmse(test$Target - Prediction_1)
#random forest model
reg2<- randomForest(Target~.,data= train)
Prediction_2 <- predict(reg2, newdata= test)
rmse(Prediction_2 - test$Target)
#load data
df <- read_csv(file.choose())
df.backup <- df
#check for missing data
missing_data <- apply(df, 2, function(x) any(is.na(x)))
print(missing_data)
#remove missing data
df <- na.omit(df)
summary(df)
#drop columns
df$PostalCode <- NULL
df$`Street 1` <- NULL
df$City <- NULL
#data transformation
df$PlaceAge <-as.integer(format(Sys.Date(), "%Y")) - df$`Year Built`
#drop year build
df$`Year Built` <- NULL
#update lease date
df$LeaseEndDate <- mdy(df$LeaseEndDate)
df$LeaseDiffDays <- as.numeric((Sys.Date() - df$LeaseEndDate))
#drop lease Date
df$LeaseEndDate <- NULL
#convert catgeorical
df_cat <- df[,c(1,2)]
df_cat <- lapply(df_cat, function(x) as.factor(as.character(x)))
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
df_cts <- df[,c(4,5,6)]
#convert chr to float
df_cts <- lapply(df_cts, function(x) as.double(as.character(x)))
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
#df_cts <- as.data.frame(lapply(df_cts, normalize))
Target <- df$RecurringCharges
df_new <- cbind(df_cat_new, df_cts, Target)
df_new[1] <- NULL
df-new[3] <- NULL
#split data
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
reg1<- lm(Target~., data = train)
summary(reg1)
#check performance of model
Prediction_1<- predict(reg1, newdata= test)
rmse(test$Target - Prediction_1)
#random forest model
reg2<- randomForest(Target~.,data= train)
summary(reg2)
#check performance
Prediction_2 <- predict(reg2, newdata= test)
rmse(Prediction_2 - test$Target)
gvlma::gvlma(reg1)
install.packages(gvlma)
install.packages("gvlma")
library(gvlma)
gvlma::gvlma(reg1)
gvlma(reg1)
gvlma(reg2)
