#remove old data
rm(list=ls())
#load libraries
for (package in c('ggplot2','corrplot','tidyverse','caret','mlbench')) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#read file
df <- read.delim(file.choose(),header=FALSE,sep="")
#update column names
names(df) <- c('age','sex','chest_pain_type','resting_blood_pressure',"serum_cholestrol",'fasting_blood_sugar',
'resting_ecg',"max_heart","exercise_angina",'oldpeak','slope_peak_exercise','major_vessels',
'thal','disease')
#check for imbalanced data
ggplot(data=df, aes(x=factor(disease))) +
geom_bar() + theme_classic() + xlab("Number of heart diseases")
df.backup <- df
#df <- df.backup
df <- df %>%
select('age',
'sex',
'max_heart',
'resting_blood_pressure', 'disease')
df$disease <- recode_factor(df$disease, "1" = "0", "2" = "1")
#remove empty data
df <- na.omit(df)
library(car)
scatterplotMatrix(df[,1:4])
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df$age <- normalize((df$age))
df$max_heart <- normalize((df$max_heart))
df$resting_blood_pressure <- normalize((df$resting_blood_pressure))
#feature selection
#Remove Redundant Feature remove when absolute correlation >= 0.75
set.seed(7)
# # calculate correlation matrix
correlationMatrix <- cor(df[,1:4])
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
print(highlyCorrelated)
#no columns to remove
#machine learing
#split data into train and test
library(caTools)
set.seed(123)
sample <- sample.split(df,SplitRatio = 0.75)
train <- subset(df,sample ==TRUE)
test <- subset(df, sample==FALSE)
#models
model_gbm<-train(train[,1:4],train[,5],method='gbm')
model_rf<-train(train[,1:4],train[,5],method='rf')
model_nnet<-train(train[,1:4],train[,5],method='nnet')
#Predictions
predictions<-predict.train(object=model_gbm,test,type="raw")
#table(predictions)
confusionMatrix(predictions,test[,5])
predictions_rf<-predict.train(object=model_rf,test,type="raw")
confusionMatrix(predictions_rf,test[,5])
predictions_nnet<-predict.train(object=model_nnet,test,type="raw")
confusionMatrix(predictions_nnet,test[,5])
#remove old data
rm(list=ls())
#load libraries
packages <- c('ggplot2','corrplot','tidyverse','caret','mlbench','mice', 'caTools','dummies')
for (package in packages) {
if (!require(package, character.only=T, quietly=T)) {
install.packages(package)
library(package, character.only=T)
}
}
#load data
df <- read.delim(file.choose(),header=FALSE,sep="")
#update column names
names(df) <- c('age','sex','chest_pain_type','resting_blood_pressure',"serum_cholestrol",'fasting_blood_sugar',
'resting_ecg',"max_heart","exercise_angina",'oldpeak','slope_peak_exercise','major_vessels',
'thal','disease')
#check for imbalanced data
ggplot(data=df, aes(x=factor(disease))) +
geom_bar() + theme_classic() + xlab("Number of heart diseases")
df.backup <- df
df.summary()
summary(df)
glimpse(df)
corinfo <- df[,1:13]
corrplot(cor(corinfo), method="number")
missing_data <- apply(df, 2, function(x) any(is.na(x))) #no missing data
print(missing_data)
df <- na.omit(df)
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
glimpse(df)
unique(df$thal)
unique(df$major_vessels)
unique(df$major_vessels)
df_cts <- df[,c(1,4,5,8,10)]
df$disease <- recode_factor(df$disease, "1" = "0", "2" = "1")
Target <- df$disease
#categorical variables
df_cts <- df[,c(2,3,6,7,9,11,12,13)]
#one hot encoding
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
#categorical variables
df_cat <- df[,c(2,3,6,7,9,11,12,13)]
#one hot encoding
df_cat_new <- dummy.data.frame(as.data.frame(df_cat), sep = "_")
#normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df_cts <- df[,c(1,4,5,8,10)]
df_cts <- as.data.frame(lapply(df_cts, normalize))
#combine data frame
df_new <- cbind(df_cat_new, df_cts,Target)
#feature selection
#Remove Redundant Feature remove when absolute correlation >= 0.75
set.seed(7)
# # calculate correlation matrix
correlationMatrix <- cor(df[,1:4])
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
print(highlyCorrelated)
#no columns to remove
# # print indexes of highly correlated attributes
print(highlyCorrelated)
#remove highly correlated columns
# # calculate correlation matrix
correlationMatrix <- cor(df_new[,1:length(df_new)-1])
# # summarize the correlation matrix
# print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# # print indexes of highly correlated attributes
print(highlyCorrelated)
#split data
set.seed(123)
sample <- sample.split(df_new,SplitRatio = 0.75)
train <- subset(df_new,sample ==TRUE)
test <- subset(df_new, sample==FALSE)
#cross fold validation
control <- trainControl(method="repeatedcv", number=10, repeats=5)
#random forest
fit.rf <- train(Target~., data=train, method="rf", trControl=control)
#boosting algorithm - Stochastic Gradient Boosting (Generalized Boosted Modeling)
fit.gbm <- train(Target~., data=train, method="gbm", trControl=control)
#svm
fit.svm <- train(Target~., data=train, method="svmRadial", trControl=control)
#------------------
#compare models
#------------------
results <- resamples(list(randomforest = fit.rf, gradboost = fit.gbm, svm = fit.svm))
summary(results)
# boxplot comparison
bwplot(results)
# Dot-plot comparison
dotplot(results)
#use test data
fit.gbm <- train(class~., data=train, method="gbm", trControl=control)
test_scores <- predict(fit.gbm, test)
confusionMatrix(test_scores, test$Glass)
test_scores <- predict(fit.gbm, test)
confusionMatrix(test_scores, test$Target)
test_scores <- predict(fit.svm, test)
confusionMatrix(test_scores, test$Target)
test_scores <- predict(fit.rf, test)
confusionMatrix(test_scores, test$Target)
print(mlbench)
print(caTools)
fit.nnet <- train(Target~., data=train, method="nnet", trControl=control)
results <- resamples(list(randomforest = fit.rf, gradboost = fit.gbm, svm = fit.svm, nnet = fit.nnet))
summary(results)
# boxplot comparison
bwplot(results)
# Dot-plot comparison
dotplot(results)
fit.abag <- train(Target~., data=train, method="adabag", trControl=control)
print(caret)
caret::compare_models()
