library(caret)
library(MASS)
library(randomForest)
library(party)
#clear old data
rm(list=ls())
#load data
genetic_data <- read.csv(file.choose())
soil_data <- read.csv(file.choose())
weather_data <- read.csv(file.choose())
validation_performance <- read.csv(file.choose())
summary(genetic_data)
summary(soil_data)
summary(weather_data)
summary(validation_performance)
View(soil_data)
View(weather_data)
View(genetic_data)
genetic_data.orig <- genetic_data
soil_data.orig <- soil_data
weather_data.orig <- weather_data
validation_performance.orig <- validation_performance
corrplot(soildata[,4:11])
corrplot(soil_data[,4:11])
#libraries
library(ggplot2)
library(corrplot)
library(lattice)
library(dplyr)
library(tidyr)
library(LogicReg)
library(caret)
library(readxl)
#==========================
#logistic regression
#==========================
#Clear old data
rm(list=ls())
mydata <- read_excel(file.choose())
#summary
summary(mydata)
#Prepare data
mydata.orig = mydata #save orig data copy
mydata <- na.omit(mydata) # listwise deletion of missing
#set seed
set.seed(1)
View(mydata)
??dummy
#libraries
library(ggplot2)
library(corrplot)
library(lattice)
library(dplyr)
library(tidyr)
library(LogicReg)
library(caret)
library(readxl)
#==========================
#logistic regression
#==========================
#Clear old data
rm(list=ls())
mydata <- read_excel(file.choose())
#summary
summary(mydata)
#Prepare data
mydata.orig = mydata #save orig data copy
mydata <- na.omit(mydata) # listwise deletion of missing
#set seed
set.seed(1)
corrplot(mydata, method="number")
#libraries
library(ggplot2)
library(corrplot)
library(lattice)
library(dplyr)
library(tidyr)
library(LogicReg)
library(caret)
library(readxl)
corrplot(mydata, method="number")
View(mydata)
View(mydata)
splitIndex <- createDataPartition(mydata$Default, p = .80,list = FALSE, times = 1)
trainSplit <- data[ splitIndex,]
testSplit <- data[-splitIndex,]
print(table(trainSplit$Default))
splitIndex <- createDataPartition(mydata$Default, p = .80,list = FALSE, times = 1)
trainSplit <- mydata[ splitIndex,]
testSplit <- mydata[-splitIndex,]
print(table(trainSplit$Default))
#logistic regression
ctrl <- trainControl(method = "cv", number = 5)
modelglm <- train(Default ~. , data = trainSplit, method = "glm", trControl = ctrl)
summary(modelglm)
predictors <- names(trainSplit)[names(trainSplit) != 'Default']
predglm <- predict(modelglm, testSplit)
summary(predglm)
confusionMatrix(predglm, testSplit$Default)
aucglm <- roc(as.numeric(testSplit$Default), as.numeric(predglm),  ci=TRUE)
plot(aucglm, ylim=c(0,1), print.thres=TRUE,
main=paste('Logistic Regression AUC:',round(aucglm$auc[[1]],3)),col = 'blue')
library(pROC)
### score prediction using AUC
confusionMatrix(predglm, testSplit$Default)
aucglm <- roc(as.numeric(testSplit$Default), as.numeric(predglm),  ci=TRUE)
plot(aucglm, ylim=c(0,1), print.thres=TRUE,
main=paste('Logistic Regression AUC:',round(aucglm$auc[[1]],3)),col = 'blue')
corrplot(soil_data[,4:11])
library(plyr)
library(corrplot)
library(ggplot2)
library(gridExtra)
library(ggthemes)
library(caret)
library(MASS)
library(randomForest)
library(party)
#Clear old data
rm(list=ls())
#use filechooser to select csv file
mydata <- read.csv(file.choose(), sep = ',')
head(mydata)
#summary
summary(mydata)
#data cleaning
#load libraries
library(cluster)
library(ggplot2)
library(factoextra)
library(corrplot)
library(psy)
library(lattice)
library(nFactors)
library(RColorBrewer)
library(scales)
#remove all data
rm(list=ls())
#use filechooser to select csv file
mydata <- read.csv(file.choose(), sep = ',')
View(mydata)
output <- lm(LIFE ~ CORRUPT, data = mydata)
summary(output)
ln(5)
log(5)
output2 <- lm(log(LIFE) ~ CORRUPT, data = mydata)
summary(output2)
resid(output) #List of residuals
plot(density(resid(output))) #A density plot
qqnorm(resid(output))
resid(output2) #List of residuals
plot(density(resid(output2))) #A density plot
qqnorm(resid(output2))
output2 <- lm(log(LIFE) ~ log(CORRUPT), data = mydata)
summary(output2)
resid(output2) #List of residuals
plot(density(resid(output2))) #A density plot
qqnorm(resid(output2))
output2 <- lm(log(CORRUPT) ~ LIFE, data = mydata)
summary(output2)
resid(output2) #List of residuals
plot(density(resid(output2))) #A density plot
qqnorm(resid(output2))
output2 <- lm(log(CORRUPT) ~ log(LIFE), data = mydata)
summary(output2)
resid(output2) #List of residuals
plot(density(resid(output2))) #A density plot
qqnorm(resid(output2))
View(mydata)
#simple linear regression
output <- lm(LSI ~ CORRUPT, data = mydata)
summary(output)
resid(output) #List of residuals
plot(density(resid(output))) #A density plot
qqnorm(resid(output))
output2 <- lm(log(LSI) ~ CORRUPT, data = mydata)
summary(output2)
resid(output2) #List of residuals
plot(density(resid(output2))) #A density plot
qqnorm(resid(output2))
#simple linear regression
output <- lm(LSI ~ CORRUPT, data = mydata)
summary(output)
resid(output) #List of residuals
plot(density(resid(output))) #A density plot
qqnorm(resid(output))
output2 <- lm(log(LSI) ~ CORRUPT, data = mydata)
summary(output2)
resid(output2) #List of residuals
plot(density(resid(output2))) #A density plot
qqnorm(resid(output2))
output2 <- lm(log(LSI) ~ log(CORRUPT), data = mydata)
summary(output2)
resid(output2) #List of residuals
plot(density(resid(output2))) #A density plot
qqnorm(resid(output2))
#remove all data
rm(list=ls())
#use filechooser to select csv file
mydata <- read.csv(file.choose(), sep = ',')
View(mydata)
#simple linear regression
output <- lm(Gesell.score ~ AGE, data = mydata)
summary(output)
resid(output) #List of residuals
plot(density(resid(output))) #A density plot
qqnorm(resid(output))
View(mydata)
#simple linear regression
output <- lm(Gesell.score ~ Age, data = mydata)
summary(output)
resid(output) #List of residuals
plot(density(resid(output))) #A density plot
qqnorm(resid(output))
#simple linear regression
output <- lm(log(Gesell.score) ~ Age, data = mydata)
summary(output)
resid(output) #List of residuals
plot(density(resid(output))) #A density plot
qqnorm(resid(output))
library(ggplot2)
ggplot(mydata, aes(x = Age, y = Gesell.score)) +
geom_boxplot()
library("readxl")
#remove all data
rm(list=ls())
mydata <- read_excel(file.choose())
str(mydata)
View(mydata)
startDate_forcast <- as.Date("20160401","%Y%m%d")
endDate_forcast <- as.Date("20160531","%Y%m%d")
history <- getDataFrame(mydata,startDate_history,endDate_history)
forcast <- getDataFrame(mydata,startDate_forcast,endDate_forcast)
history <- GetDataFrame(mydata,startDate_history,endDate_history)
forcast <- GetDataFrame(mydata,startDate_forcast,endDate_forcast)
#remove all data
rm(list=ls())
#use filechooser to select csv file
mydata <- read.csv(file.choose(), sep = ',')
str(mydata)
View(mydata)
library(corrplot)
corrplot(mydata[1,2],'number')
corrplot(mydata[,1:2],'number')
corrplot(mydata[,],'number')
corrplot(mydata[,1],'number')
corrplot(mydata[,1:2],'number')
summary(mydata[,1:2])
age_writer <- mydata[,1:2]
corrplot(age_writer,method = "number")
#simple linear regression
output <- lm(mydata$agedeath ~ mydata$writer, data = mydata)
summary(output)
resid(output) #List of residuals
plot(density(resid(output))) #A density plot
qqnorm(resid(output))
model1<- aov(writer~agedeath)
summary(model1)
#anova
model1<- aov(mydata$writer~mydata$agedeath)
summary(model1)
TukeyHSD(model1, conf.level = 0.99)
plot(TukeyHSD(model1, conf.level = 0.99),las=1, col = "red")
par(mfrow=c(2,2))
plot(model1)
#normality chck
uhat<-resid(model1)
shapiro.test(uhat)
#remove all data
rm(list=ls())
#use filechooser to select csv file
mydata <- read.csv(file.choose(), sep = ',')
str(mydata)
summary(mydata)
View(mydata)
library("ggpubr")
View(mydata)
ggboxplot(my_data, x = "materials", y = "gpi", color = "supp",
palette = c("#00AFBB", "#E7B800"))
ggboxplot(mydata, x = "materials", y = "gpi", color = "supp",
palette = c("#00AFBB", "#E7B800"))
ggboxplot(mydata, x = "material", y = "gpi", color = "supp",
palette = c("#00AFBB", "#E7B800"))
ggboxplot(mydata, x = "material", y = "gpi", color = "group",
palette = c("#00AFBB", "#E7B800"))
library("ggpubr")
ggboxplot(mydata, x = "material", y = "gpi", color = "group",
palette = c("#00AFBB", "#E7B800","#00ABBB","#E7E800","#00AFBF","#E7F800"))
res.aov2 <- aov(gpi ~ material + group, data = mydata)
summary(res.aov2)
res.aov3 <- aov(gpi ~ material + group, data = mydata)
res.aov3 <- aov(gpi ~ material + group + material:group, data = mydata)
summary(res.aov3)
#remove all data
rm(list=ls())
#use filechooser to select csv file
mydata <- read.csv(file.choose(), sep = ',')
str(mydata)
summary(mydata)
View(mydata)
library("ggpubr")
ggboxplot(mydata, x = "post", y = "pre", color = "group",
palette = c("#00AFBB", "#E7B800","#00BBBB"))
library(ggplot2)
ggplot2.scatterplot(data=mydata, xName='pre',yName='post',
groupName="group")
install.packages("easyGgplot2")
library(easyGgplot2)
ggplot2.scatterplot(data=mydata, xName='pre',yName='post',
groupName="group")
library(ggplot2)
ggplot(mydata, aes(pre, post)) + geom_point(aes(colour = factor(cyl)))
library(ggplot2)
ggplot(mydata, aes(pre, post)) + geom_point(aes(colour = factor(group)))
ggplot(mydata, aes(pre, post)) + geom_boxplot(colour = factor(group)))
ggplot(mydata, aes(pre, post)) + geom_boxplot(colour = factor(group))
ggplot(mydata, aes(pre, group)) + geom_boxplot()
ggplot(mydata, aes(group, pre)) + geom_boxplot()
output <- wilcox.test(pre, post)
output <- wilcox.test(mydata$pre, mydata$post)
res <- wilcox.test(weight ~ group, data = mydata,
exact = FALSE)
res <- wilcox.test(pre ~ group, data = mydata,
exact = FALSE)
res <- wilcox.test(pre,post ~ group, data = mydata,
exact = FALSE)
res <- wilcox.test(post ~ group, data = mydata,
exact = FALSE)
#remove all data
rm(list=ls())
#use filechooser to select csv file
mydata <- read.csv(file.choose(), sep = ',')
str(mydata)
summary(mydata)
output <- kruskal.test(strength ~ weeks, data = mydata)
print(output)
plastics_data <- plastics
??plastics
library(fma)
plastics_data <- plastics
prod_A_ts = ts(plastics, start = 1995, frequency = 12)
plot(prod_A_ts)
prod_A_ts = ts(plastics, start = 1, frequency = 12)
plot(prod_A_ts)
prod_A_ts = ts(plastics, start = 1, frequency = 1)
plot(prod_A_ts)
prod_A_ts = ts(plastics, start = "Jan", frequency = 1)
plot(prod_A_ts)
prod_A_ts = ts(plastics, start = 1, frequency = 1)
plot(prod_A_ts)
fit <- decompose(plastics, type="multiplicative")
plot(fit)
fit <- decompose(prod_A_ts, type="multiplicative")
plot(fit)
#load libraries
library(tidyverse)
library(neuralnet)
library(GGally)
#load data
url <- 'http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data'
Yacht_Data <- read_table(file = url,
col_names = c('LongPos_COB', 'Prismatic_Coeff',
'Len_Disp_Ratio', 'Beam_Draut_Ratio',
'Length_Beam_Ratio','Froude_Num',
'Residuary_Resist')) %>% na.omit()
View(Yacht_Data)
ggpairs(Yacht_Data, title = "Scatterplot Matrix of the Features of the Yacht Data Set")
# Scale the Data
scale01 <- function(x){
(x - min(x)) / (max(x) - min(x))
}
Yacht_Data <- Yacht_Data %>%
mutate_all(scale01)
# Split into test and train sets
set.seed(12345)
Yacht_Data_Train <- sample_frac(tbl = Yacht_Data, replace = FALSE, size = 0.80)
Yacht_Data_Test <- anti_join(Yacht_Data, Yacht_Data_Train)
#1 hidden layer ANN
set.seed(12321)
Yacht_NN1 <- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff +
Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio +
Froude_Num, data = Yacht_Data_Train)
library(neuralnet)
install.packages("neuralnet")
library(neuralnet)
#1 hidden layer ANN
set.seed(12321)
Yacht_NN1 <- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff +
Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio +
Froude_Num, data = Yacht_Data_Train)
plot(Yacht_NN1, rep = 'best')
#SSE
NN1_Train_SSE <- sum((Yacht_NN1$net.result - Yacht_Data_Train[, 7])^2)/2
paste("SSE: ", round(NN1_Train_SSE, 4))
#use test data
Test_NN1_Output <- compute(Yacht_NN1, Yacht_Data_Test[, 1:6])$net.result
NN1_Test_SSE <- sum((Test_NN1_Output - Yacht_Data_Test[, 7])^2)/2
NN1_Test_SSE
library(tidyverse)
library(neuralnet)
library(GGally)
url <- 'http://archive.ics.uci.edu/ml/machine-learning-databases//haberman/haberman.data'
Hab_Data <- read_csv(file = url,
col_names = c('Age', 'Operation_Year',
'Number_Pos_Nodes','Survival')) %>%
na.omit() %>%
mutate(Survival = ifelse(Survival == 2, 0, 1),
Survival = factor(Survival))
#explore data
ggpairs(Hab_Data, title = "Scatterplot Matrix of the Features of the Haberman's Survival Data Set")
scale01 <- function(x){
(x - min(x)) / (max(x) - min(x))
}
Hab_Data <- Hab_Data %>%
mutate(Age = scale01(Age),
Operation_Year = scale01(Operation_Year),
Number_Pos_Nodes = scale01(Number_Pos_Nodes),
Survival = as.numeric(Survival)-1)
Hab_Data <- Hab_Data %>%
mutate(Survival = as.integer(Survival) - 1,
Survival = ifelse(Survival == 1, TRUE, FALSE))
#1 hidden layer 1 neuron ANN
set.seed(123)
Hab_NN1 <- neuralnet(Survival ~ Age + Operation_Year + Number_Pos_Nodes,
data = Hab_Data,
linear.output = FALSE,
err.fct = 'ce',
likelihood = TRUE)
lot(Hab_NN1, rep = 'best')
plot(Hab_NN1, rep = 'best')
#check errors
Hab_NN1_Train_Error <- Hab_NN1$result.matrix[1,1]
paste("CE Error: ", round(Hab_NN1_Train_Error, 3))
Hab_NN1_AIC <- Hab_NN1$result.matrix[4,1]
paste("AIC: ", round(Hab_NN1_AIC,3))
Hab_NN2_BIC <- Hab_NN1$result.matrix[5,1]
paste("BIC: ", round(Hab_NN2_BIC, 3))
library(tidyverse)
library(nycflights13)
monthdayfilter <- filter(flight,month ==1, day == 1)
monthdayfilter <- filter(flights,month ==1, day == 1)
dec25 <- filter(flights, month==12, day==25)
arranged <- arrange(flights, year, month, day)
arranged <- arrange(flights, year, month, desc(day))
selected <- select(flights, year)
selectedsome2 <- select(flights, -(year:day))
selectedsome2 <- select(flights, starts_with("abc"))
library(datasets)
head(datasets)
head(airquality)
library(dplyr)
airqualityfilter <- filter(airquality, > 70)
airqualityfilter <- filter(airquality, Temp > 70)
airqualityfilter <- filter(airquality, Temp > 70 & Month = 5)
airqualityfilter <- filter(airquality, Temp > 70 & Month == 5)
mutate(airquality, TempInC = (Temp - 32) * 5 / 9)
summarise(airquality, mean(Temp, na.rm = TRUE))
summarise(group_by(airquality, month),mean(Temp, na.rm = TRUE))
summarise(group_by(airquality, Month),mean(Temp, na.rm = TRUE))
sample(airquality, size = 15)
sample_n(airquality, size = 15)
#remove all data
rm(list=ls())
library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(lubridate)
library(arules)
library(arulesViz)
library(plyr)
retail <- read_excel('Online_retail.xlsx')
retail <- retail[complete.cases(retail), ]
retail %>% mutate(Description = as.factor(Description))
retail %>% mutate(Country = as.factor(Country))
retail$Date <- as.Date(retail$InvoiceDate)
retail$Time <- format(retail$InvoiceDate,"%H:%M:%S")
retail$InvoiceNo <- as.numeric(as.character(retail$InvoiceNo))
glimpse(retail)
setwd("~/Documents/Coding/R/R_analytics/groceries")
#data management
retail <- read_excel('Online_retail.xlsx')
retail <- retail[complete.cases(retail), ]
retail %>% mutate(Description = as.factor(Description))
retail %>% mutate(Country = as.factor(Country))
retail$Date <- as.Date(retail$InvoiceDate)
retail$Time <- format(retail$InvoiceDate,"%H:%M:%S")
retail$InvoiceNo <- as.numeric(as.character(retail$InvoiceNo))
glimpse(retail)
#data management
retail <- read_excel('Online retail.xlsx')
retail <- retail[complete.cases(retail), ]
retail %>% mutate(Description = as.factor(Description))
retail %>% mutate(Country = as.factor(Country))
retail$Date <- as.Date(retail$InvoiceDate)
retail$Time <- format(retail$InvoiceDate,"%H:%M:%S")
retail$InvoiceNo <- as.numeric(as.character(retail$InvoiceNo))
glimpse(retail)
#what time do people buy
retail$Time <- as.factor(retail$Time)
a %>%
ggplot(aes(x=Time)) +
geom_histogram(stat="count",fill="indianred")
#How many items each customer buy?
detach("package:plyr", unload=TRUE)
retail %>%
group_by(InvoiceNo) %>%
summarize(n_items = mean(Quantity)) %>%
ggplot(aes(x=n_items))+
geom_histogram(fill="indianred", bins = 100000) +
geom_rug()+
coord_cartesian(xlim=c(0,80))
